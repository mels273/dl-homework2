{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d92cd5",
   "metadata": {},
   "source": [
    "### Question 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf7c164",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96951ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_rnacompete_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c528d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Metadata from metadata.xlsx...\n",
      "  > Metadata loaded in 0.10 seconds.\n",
      "Loading Data from norm_data.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mattia/Documents/IST/DL/dl-homework2/q2/venv/lib/python3.12/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Data Matrix loaded in 36.73 seconds.\n",
      "Saving processed data to data/RBFOX1_train_data.pt...\n",
      "Loading Metadata from metadata.xlsx...\n",
      "  > Metadata loaded in 0.03 seconds.\n",
      "Loading Data from norm_data.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mattia/Documents/IST/DL/dl-homework2/q2/venv/lib/python3.12/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Data Matrix loaded in 35.63 seconds.\n",
      "Saving processed data to data/RBFOX1_val_data.pt...\n",
      "Loading Metadata from metadata.xlsx...\n",
      "  > Metadata loaded in 0.02 seconds.\n",
      "Loading Data from norm_data.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mattia/Documents/IST/DL/dl-homework2/q2/venv/lib/python3.12/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Data Matrix loaded in 34.81 seconds.\n",
      "Saving processed data to data/RBFOX1_test_data.pt...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ds_train = load_rnacompete_data(\"RBFOX1\", split=\"train\")\n",
    "ds_val = load_rnacompete_data(\"RBFOX1\", split=\"val\")\n",
    "ds_test = load_rnacompete_data(\"RBFOX1\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c4c087",
   "metadata": {},
   "source": [
    "##### CNN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af921b",
   "metadata": {},
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aef565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simple CNN model inspired by DeepBind (Alipanahi et al, 2015)\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, motif_len=24, num_filters=64, hidden=32, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, num_filters, kernel_size=(motif_len, 4))\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(num_filters, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x).squeeze(-1))   # (B, C, L')\n",
    "        x = torch.max(x, dim=2).values         # global max pool -> (B, C)\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)                        # linear output\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad2a5032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1,1,41,4)\n",
    "model = CNN().to(\"cpu\")\n",
    "out = model(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19edee52",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a4f634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import masked_mse_loss, masked_spearman_correlation, configure_seed\n",
    "\n",
    "\n",
    "def train(model, epochs, batch_size, lr, ds_train, ds_val, checkpoint_path, verbose=True, seed=None):\n",
    "    \"\"\"\n",
    "    Trains model with specified hyperparams on given train and validation datasets.\n",
    "    Saves model checkpoint to specified path. \n",
    "    \"\"\"\n",
    "    \n",
    "    if seed is not None:\n",
    "        configure_seed(seed)\n",
    "        \n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(ds_val, batch_size=len(ds_val), shuffle=False)\n",
    "\n",
    "    # Optimizer\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Save best state\n",
    "    best_state_dict = {}\n",
    "    best_val_corr = 0.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    # Training loop \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        train_corr = 0.0\n",
    "        \n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "            \n",
    "        # Loop over batches\n",
    "        for x, y, mask in train_loader:\n",
    "            \n",
    "            # Reset optimizer\n",
    "            optim.zero_grad()\n",
    "            \n",
    "            # Add channel dimension to x\n",
    "            x = x.unsqueeze(1)\n",
    "            \n",
    "            # Forward pass\n",
    "            y_hat = model(x)\n",
    "            \n",
    "            # Loss\n",
    "            loss = masked_mse_loss(y_hat, y, mask)\n",
    "            train_loss += loss.item()\n",
    "            train_corr += masked_spearman_correlation(y_hat, y, mask)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Optimizer step\n",
    "            optim.step()\n",
    "            \n",
    "        \n",
    "        # Average  \n",
    "        train_loss /= len(train_loader)\n",
    "        train_corr /= len(train_loader)\n",
    "        \n",
    "        # Set model to evalution mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Validation\n",
    "        val_corr = 0.0\n",
    "        for x, y, mask in val_loader:\n",
    "            \n",
    "            # Add channel dimension to x \n",
    "            x = x.unsqueeze(1)\n",
    "            \n",
    "            # Compute spearman correlation\n",
    "            val_corr += masked_spearman_correlation( model(x), y, mask)\n",
    "            \n",
    "        # Average\n",
    "        val_corr /= len(val_loader)\n",
    "        \n",
    "        # Update best state\n",
    "        if val_corr > best_val_corr:\n",
    "            best_val_corr = val_corr\n",
    "            best_state_dict = model.state_dict()\n",
    "            best_epoch = epoch\n",
    "            \n",
    "            # Save it to file\n",
    "            with open(checkpoint_path,\"wb\") as file:\n",
    "                pickle.dump(best_state_dict, file)\n",
    "            \n",
    "        if verbose:\n",
    "            print(f\"Epoch: {epoch}, Train loss: {train_loss}, Val corr: {val_corr }\")\n",
    "      \n",
    "    if verbose:  \n",
    "        print(f\"Best model at epoch {best_epoch} with validation spearman correlation {best_val_corr}\")\n",
    "    \n",
    "    return best_val_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e5cdcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 0.9461544831172225, Val corr: 0.4329873025417328\n",
      "Epoch: 1, Train loss: 0.7004308587518232, Val corr: 0.4810325801372528\n",
      "Epoch: 2, Train loss: 0.6235661557245634, Val corr: 0.4916272759437561\n",
      "Epoch: 3, Train loss: 0.6062124314611723, Val corr: 0.49794822931289673\n",
      "Epoch: 4, Train loss: 0.5868577840947978, Val corr: 0.5055224299430847\n",
      "Epoch: 5, Train loss: 0.5856072598332119, Val corr: 0.5067200660705566\n",
      "Epoch: 6, Train loss: 0.5817493397139744, Val corr: 0.5134938359260559\n",
      "Epoch: 7, Train loss: 0.5664813454690283, Val corr: 0.5118592381477356\n",
      "Epoch: 8, Train loss: 0.5593826341534167, Val corr: 0.5156391263008118\n",
      "Epoch: 9, Train loss: 0.5562774644922515, Val corr: 0.513916552066803\n",
      "Epoch: 10, Train loss: 0.5538510314191368, Val corr: 0.5178666114807129\n",
      "Epoch: 11, Train loss: 0.5540623706357864, Val corr: 0.5205375552177429\n",
      "Epoch: 12, Train loss: 0.5545746827947682, Val corr: 0.5219163298606873\n",
      "Epoch: 13, Train loss: 0.5487678908700968, Val corr: 0.5255662202835083\n",
      "Epoch: 14, Train loss: 0.5383630732484458, Val corr: 0.5236363410949707\n",
      "Epoch: 15, Train loss: 0.5390647900357487, Val corr: 0.523220419883728\n",
      "Epoch: 16, Train loss: 0.5356024445209958, Val corr: 0.5271587371826172\n",
      "Epoch: 17, Train loss: 0.538525719383351, Val corr: 0.5288452506065369\n",
      "Epoch: 18, Train loss: 0.5302284018430533, Val corr: 0.5252503156661987\n",
      "Epoch: 19, Train loss: 0.5330972916567673, Val corr: 0.5283539295196533\n",
      "Epoch: 20, Train loss: 0.5284230107891148, Val corr: 0.52564537525177\n",
      "Epoch: 21, Train loss: 0.5275758311191984, Val corr: 0.5282623767852783\n",
      "Epoch: 22, Train loss: 0.5268443414758625, Val corr: 0.5246750712394714\n",
      "Epoch: 23, Train loss: 0.52790966819705, Val corr: 0.5319175720214844\n",
      "Epoch: 24, Train loss: 0.5234914085672135, Val corr: 0.5269644856452942\n",
      "Epoch: 25, Train loss: 0.5292478596658226, Val corr: 0.5285301208496094\n",
      "Epoch: 26, Train loss: 0.5294333563559251, Val corr: 0.531108021736145\n",
      "Epoch: 27, Train loss: 0.518562303692972, Val corr: 0.5304151177406311\n",
      "Epoch: 28, Train loss: 0.5226030161589463, Val corr: 0.5370768904685974\n",
      "Epoch: 29, Train loss: 0.522212734905415, Val corr: 0.5372030735015869\n",
      "Best model at epoch 29 with validation spearman correlation 0.5372030735015869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.5372)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Device\n",
    "device = \"cpu\"\n",
    "\n",
    "# Model \n",
    "model = CNN().to(device)\n",
    "\n",
    "# Train\n",
    "train(model, \n",
    "      epochs=30, \n",
    "      batch_size=256, \n",
    "      lr=1e-3,\n",
    "      ds_train=ds_train,\n",
    "      ds_val=ds_val,\n",
    "      checkpoint_path=\"q2_cnn_best.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6eb238",
   "metadata": {},
   "source": [
    "Evaluate on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb31d15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation on test set is 0.45878487825393677\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate on test dataset\n",
    "model = CNN().to(device)\n",
    "\n",
    "# Data loader for test dataset\n",
    "test_loader = DataLoader(ds_test, len(ds_test), shuffle=False)\n",
    "\n",
    "with open(\"q2_cnn_best.model\",\"rb\") as file:\n",
    "    model.load_state_dict(pickle.load(file))\n",
    "    \n",
    "test_corr = 0.0\n",
    "for x, y, mask in test_loader:\n",
    "        \n",
    "    # Add channel dimension to x \n",
    "    x = x.unsqueeze(1)\n",
    "    \n",
    "    # Compute spearman correlation\n",
    "    test_corr = masked_spearman_correlation( model(x), y, mask)\n",
    "    \n",
    "    # Break (single batch)\n",
    "    break\n",
    "\n",
    "print(f\"Spearman correlation on test set is {test_corr}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ffa32",
   "metadata": {},
   "source": [
    "Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dfc68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4d00c87",
   "metadata": {},
   "source": [
    "##### RNN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a889be1",
   "metadata": {},
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f53b0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "# Simple RNN inspired by DeeperBind ( ..., 2016)\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: int = 41,          # kept for API compatibility; not strictly needed\n",
    "        motif_len: int = 11,           # paper commonly uses motif length ~11\n",
    "        conv_out_ch: int = 32,         # number of motif detectors / filters\n",
    "        lstm_hidden_size: int = 128,\n",
    "        lstm_layers: int = 2,\n",
    "        dropout: float = 0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Single convolutional motif detector bank (spans full alphabet width=4)\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=conv_out_ch,\n",
    "            kernel_size=(motif_len, 4)\n",
    "        )\n",
    "\n",
    "        # Stacked LSTM over the sequence of conv features\n",
    "        # batch_first=True so input/output is (B, T, C)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=conv_out_ch,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if lstm_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        # Small MLP head (at most one hidden layer, as described in the paper)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(lstm_hidden_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Expected x shape: (B, 1, L, 4)  (one-hot sequence with channel dim)\n",
    "        Returns: (B, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # Conv + ReLU -> (B, C, T, 1) then squeeze -> (B, C, T)\n",
    "        z = F.relu(self.conv(x).squeeze(-1))\n",
    "\n",
    "        # Prepare for LSTM: (B, T, C)\n",
    "        v = z.transpose(1, 2).contiguous()\n",
    "\n",
    "        # LSTM over positions\n",
    "        out, _ = self.lstm(v)          # out: (B, T, H)\n",
    "\n",
    "        # \"Last unrolled LSTM block makes the final decision\"\n",
    "        h = out[:, -1, :]              # (B, H)\n",
    "\n",
    "        # MLP head (linear output for regression)\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = self.dropout(h)\n",
    "        y = self.fc2(h)                # (B, 1)\n",
    "\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5bf2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1,1,41,4)\n",
    "model = RNN().to(\"cpu\")\n",
    "\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c348c1",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae81a8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 1.003463015669868, Val corr: -0.2452460527420044\n",
      "Epoch: 1, Train loss: 0.9938512233199266, Val corr: 0.20198918879032135\n",
      "Epoch: 2, Train loss: 0.9459036129807669, Val corr: 0.27277666330337524\n",
      "Epoch: 3, Train loss: 0.8337126538236305, Val corr: 0.4138477146625519\n",
      "Epoch: 4, Train loss: 0.6838430880554138, Val corr: 0.4574824571609497\n",
      "Epoch: 5, Train loss: 0.6046485933204176, Val corr: 0.4754132926464081\n",
      "Epoch: 6, Train loss: 0.5500769388108027, Val corr: 0.4974198043346405\n",
      "Epoch: 7, Train loss: 0.5298565075826392, Val corr: 0.5067995190620422\n",
      "Epoch: 8, Train loss: 0.5172585751959886, Val corr: 0.5115245580673218\n",
      "Epoch: 9, Train loss: 0.5166842547989396, Val corr: 0.5152186155319214\n",
      "Epoch: 10, Train loss: 0.4981591330949592, Val corr: 0.5267212986946106\n",
      "Epoch: 11, Train loss: 0.49201246804345855, Val corr: 0.5272833108901978\n",
      "Epoch: 12, Train loss: 0.4811716394291984, Val corr: 0.5358620882034302\n",
      "Epoch: 13, Train loss: 0.4801191819644479, Val corr: 0.5370001792907715\n",
      "Epoch: 14, Train loss: 0.468015837685141, Val corr: 0.5409587025642395\n",
      "Epoch: 15, Train loss: 0.46466957466312186, Val corr: 0.5430729389190674\n",
      "Epoch: 16, Train loss: 0.45511972596721045, Val corr: 0.5391237735748291\n",
      "Epoch: 17, Train loss: 0.47830990445676935, Val corr: 0.5528964400291443\n",
      "Epoch: 18, Train loss: 0.5000292998812501, Val corr: 0.548213005065918\n",
      "Epoch: 19, Train loss: 0.44918596886453177, Val corr: 0.5584108233451843\n",
      "Best model at epoch 19 with validation spearman correlation 0.5584108233451843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.5584)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device \n",
    "device = \"cpu\"\n",
    "\n",
    "# Model\n",
    "model = RNN().to(device)\n",
    "\n",
    "train(model, 20, 512, 1e-3, ds_train, ds_val, \"q2_rnn_best.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580a7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
